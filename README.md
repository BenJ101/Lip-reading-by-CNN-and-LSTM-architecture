# Lip-reading-by-CNN-and-LSTM-architecture

## Introduction

  This deep learning project is about Lip Reading which is a technique of understanding speech by visually interpreting the movements of the lips so, we implemented this Lip Reading by using deep learning. It can be used for hard-of hearing people or to get some information from video without sound.
  
### Objective

  Lip Reading relies on the kind of the language and, in this project, **we chose Hangul as the language to implement the Lip Reading**.

  Because there were no Hangul lip datasets available for deep learning, it was necessary to create the datasets manually. Therefore, to recognize the overall general terms of language, not only does it require a large number of datasets, but also the size and complexity of the neural net increases. Due to time and hardware limitations, **this project defined the problem by classifying only a few words**.
  
### Precedent research & Reference

  There are some precedent researches which are related to this project as follows :
  
  * Garg Amit, Jonathan Noyola, and Sameep Bagadia. Lip reading using CNN and LSTM. Technical report, Stanford University, CS231n project report, 2016
  
  * Gutierrez, Abiel, and Zoe-Alanah Robert. "Lip Reading Word Classification."
  
  * Parth Khetarpal, Shayan Sadar, and Riaz Moradian. "LipVision: A Deep Learning Approachâ€œ, International Journal of Computer Applications, 2017

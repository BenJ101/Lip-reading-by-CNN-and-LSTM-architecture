{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Load keras package needed\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import os # drectory library\n",
    "import cv2 # image processing library\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Input\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.layers import TimeDistributed\n",
    "from keras import applications\n",
    "from keras import optimizers\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "# Fix random seed\n",
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 28 # input frame numbers for LSTM\n",
    "n_labels = 4 # Number of Dataset Labels\n",
    "Learning_rate = 0.0001 # Oprimizers lr, in this case, for adam\n",
    "batch_size = 32\n",
    "validation_ratio = 0.2 \n",
    "num_epochs = 10\n",
    "img_col = 128 # Transfer model input size ( MobileNet )\n",
    "img_row = 128 # Transfer model input size ( MobileNet )\n",
    "img_channel = 3 # RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Data is:  200\n",
      "Dataset shape is (200, 28, 128, 128, 3) (size, timestep, column, row, channel)\n",
      "Label shape is (200, 4) (size, label onehot vector)\n"
     ]
    }
   ],
   "source": [
    "# 1. Creating Datasets\n",
    "# define temporary empty list for load\n",
    "data = []\n",
    "label = []\n",
    "Totalnb = 0\n",
    "path = 'C:\\Intro to AI\\LipReading\\lipread_mp4'\n",
    "folder = 'test'\n",
    "# loop through each label (i)\n",
    "    # each label look at train folder\n",
    "    # in train folder loop through mp4 files (j) file\n",
    "    # create a folder and add that mp4 and metadata to a folder\n",
    "            # create jpg files from mp4 (k) frame in the folder\n",
    "            \n",
    "# Load Dataset\n",
    "path = 'C:\\Intro to AI\\LipReading\\lipread_mp4'\n",
    "# Loop through all labels\n",
    "for i, ind_label in enumerate(os.listdir(path)):\n",
    "    # go through train folder\n",
    "    for j, file in enumerate(os.listdir(os.path.join(path, ind_label, folder))):\n",
    "        # loop through all jpgs in folder\n",
    "        temp = []\n",
    "        jpgs = os.listdir(os.path.join(path, ind_label, folder, file))\n",
    "        jpgs = [f for f in jpgs if f.endswith('.jpg')]\n",
    "        for k, frame in enumerate(jpgs):\n",
    "            name = os.path.join(path, ind_label, folder, file, frame)\n",
    "            img = cv2.imread(name)\n",
    "            res = cv2.resize(img, dsize=(img_col, img_row), interpolation=cv2.INTER_CUBIC)\n",
    "            temp.append(res)\n",
    "        label.append(i)        \n",
    "        data.append(temp)\n",
    "        Totalnb += 1\n",
    "print(\"Total Number of Data is: \", Totalnb)\n",
    "\n",
    "# Convert List to numpy array, for Keras use\n",
    "Train_label = np.eye(n_labels)[label] # One-hot encoding by np array function\n",
    "Train_data = np.array(data)\n",
    "print(\"Dataset shape is\",Train_data.shape, \"(size, timestep, column, row, channel)\")\n",
    "print(\"Label shape is\",Train_label.shape,\"(size, label onehot vector)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling dataset for input fit function\n",
    "# if don`t, can`t train model entirely\n",
    "x = np.arange(Train_label.shape[0])\n",
    "np.random.shuffle(x)\n",
    "# same order shuffle is needed\n",
    "Train_label = Train_label[x]\n",
    "Train_data = Train_data[x]\n",
    "\n",
    "# declare data for training and validation, if you want, you can seperate testset from this\n",
    "X_train=Train_data[0:Totalnb,:]\n",
    "Y_train=Train_label[0:Totalnb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "`load_weights` requires h5py package when loading weights from HDF5. Try installing h5py.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m video \u001b[39m=\u001b[39m Input(shape\u001b[39m=\u001b[39m(timesteps,img_col,img_row,img_channel))\n\u001b[0;32m      4\u001b[0m \u001b[39m# Load transfer learning model that you want\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m model \u001b[39m=\u001b[39m applications\u001b[39m.\u001b[39;49mResNet101(input_shape\u001b[39m=\u001b[39;49m(img_col,img_row,img_channel), weights\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mimagenet\u001b[39;49m\u001b[39m'\u001b[39;49m, include_top\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m      6\u001b[0m model\u001b[39m.\u001b[39mtrainable \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m# FC Dense Layer\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\benjo\\anaconda3\\envs\\lipreading\\lib\\site-packages\\keras\\applications\\resnet.py:556\u001b[0m, in \u001b[0;36mResNet101\u001b[1;34m(include_top, weights, input_tensor, input_shape, pooling, classes, **kwargs)\u001b[0m\n\u001b[0;32m    553\u001b[0m     x \u001b[39m=\u001b[39m stack1(x, \u001b[39m256\u001b[39m, \u001b[39m23\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconv4\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    554\u001b[0m     \u001b[39mreturn\u001b[39;00m stack1(x, \u001b[39m512\u001b[39m, \u001b[39m3\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconv5\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 556\u001b[0m \u001b[39mreturn\u001b[39;00m ResNet(\n\u001b[0;32m    557\u001b[0m     stack_fn,\n\u001b[0;32m    558\u001b[0m     \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    559\u001b[0m     \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    560\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mresnet101\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    561\u001b[0m     include_top,\n\u001b[0;32m    562\u001b[0m     weights,\n\u001b[0;32m    563\u001b[0m     input_tensor,\n\u001b[0;32m    564\u001b[0m     input_shape,\n\u001b[0;32m    565\u001b[0m     pooling,\n\u001b[0;32m    566\u001b[0m     classes,\n\u001b[0;32m    567\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    568\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\benjo\\anaconda3\\envs\\lipreading\\lib\\site-packages\\keras\\applications\\resnet.py:238\u001b[0m, in \u001b[0;36mResNet\u001b[1;34m(stack_fn, preact, use_bias, model_name, include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation, **kwargs)\u001b[0m\n\u001b[0;32m    231\u001b[0m         file_hash \u001b[39m=\u001b[39m WEIGHTS_HASHES[model_name][\u001b[39m1\u001b[39m]\n\u001b[0;32m    232\u001b[0m     weights_path \u001b[39m=\u001b[39m data_utils\u001b[39m.\u001b[39mget_file(\n\u001b[0;32m    233\u001b[0m         file_name,\n\u001b[0;32m    234\u001b[0m         BASE_WEIGHTS_PATH \u001b[39m+\u001b[39m file_name,\n\u001b[0;32m    235\u001b[0m         cache_subdir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodels\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    236\u001b[0m         file_hash\u001b[39m=\u001b[39mfile_hash,\n\u001b[0;32m    237\u001b[0m     )\n\u001b[1;32m--> 238\u001b[0m     model\u001b[39m.\u001b[39;49mload_weights(weights_path)\n\u001b[0;32m    239\u001b[0m \u001b[39melif\u001b[39;00m weights \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    240\u001b[0m     model\u001b[39m.\u001b[39mload_weights(weights)\n",
      "File \u001b[1;32mc:\\Users\\benjo\\anaconda3\\envs\\lipreading\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\benjo\\anaconda3\\envs\\lipreading\\lib\\site-packages\\keras\\engine\\training.py:2920\u001b[0m, in \u001b[0;36mModel.load_weights\u001b[1;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[0;32m   2918\u001b[0m status \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   2919\u001b[0m \u001b[39mif\u001b[39;00m h5py \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 2920\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[0;32m   2921\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`load_weights` requires h5py package when loading weights \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2922\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfrom HDF5. Try installing h5py.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2923\u001b[0m     )\n\u001b[0;32m   2924\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_graph_network \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilt:\n\u001b[0;32m   2925\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2926\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to load weights saved in HDF5 format into a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2927\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msubclassed Model which has not created its variables yet. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2928\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCall the Model first, then load the weights.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2929\u001b[0m     )\n",
      "\u001b[1;31mImportError\u001b[0m: `load_weights` requires h5py package when loading weights from HDF5. Try installing h5py."
     ]
    }
   ],
   "source": [
    "# 2. Buliding a Model\n",
    "# declare input layer for CNN+LSTM architecture\n",
    "video = Input(shape=(timesteps,img_col,img_row,img_channel))\n",
    "# Load transfer learning model that you want\n",
    "model = applications.ResNet101(input_shape=(img_col,img_row,img_channel), weights='imagenet', include_top=False)\n",
    "model.trainable = False\n",
    "# FC Dense Layer\n",
    "x = model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "x = Dropout(0.3)(x)\n",
    "cnn_out = Dense(128, activation=\"relu\")(x)\n",
    "# Construct CNN model \n",
    "Lstm_inp = Model(inputs=model.input, outputs=cnn_out)\n",
    "# Distribute CNN output by timesteps \n",
    "encoded_frames = TimeDistributed(Lstm_inp)(video)\n",
    "# Contruct LSTM model \n",
    "encoded_sequence = LSTM(256)(encoded_frames)\n",
    "hidden_Drop = Dropout(0.3)(encoded_sequence)\n",
    "hidden_layer = Dense(128, activation=\"relu\")(encoded_sequence)\n",
    "outputs = Dense(n_labels, activation=\"softmax\")(hidden_layer)\n",
    "# Contruct CNN+LSTM model \n",
    "model = Model([video], outputs)\n",
    "# 3. Setting up the Model Learning Process\n",
    "# Model Compile \n",
    "adam = keras.optimizers.Adam(lr=Learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=adam, metrics=[\"accuracy\"])\n",
    "\n",
    "# 4. Training the Model\n",
    "hist = model.fit(X_train, Y_train, batch_size=batch_size, validation_split=validation_ratio, shuffle=True, epochs=num_epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential \n",
    "from keras.layers import Conv3D, LSTM, Dense, Dropout, Bidirectional, MaxPool3D, Activation, Reshape, SpatialDropout3D, BatchNormalization, TimeDistributed, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_6 (Conv3D)           (None, 28, 128, 128, 128  10496     \n",
      "                             )                                   \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 28, 128, 128, 128  0         \n",
      "                             )                                   \n",
      "                                                                 \n",
      " max_pooling3d_6 (MaxPooling  (None, 28, 64, 64, 128)  0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_7 (Conv3D)           (None, 28, 64, 64, 256)   884992    \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 28, 64, 64, 256)   0         \n",
      "                                                                 \n",
      " max_pooling3d_7 (MaxPooling  (None, 28, 32, 32, 256)  0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_8 (Conv3D)           (None, 28, 32, 32, 75)    518475    \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 28, 32, 32, 75)    0         \n",
      "                                                                 \n",
      " max_pooling3d_8 (MaxPooling  (None, 28, 16, 16, 75)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDis  (None, 28, 19200)        0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " bidirectional_4 (Bidirectio  (None, 28, 256)          19792896  \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 28, 256)           0         \n",
      "                                                                 \n",
      " bidirectional_5 (Bidirectio  (None, 28, 256)          394240    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 28, 256)           0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 28, 5)             1285      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,602,384\n",
      "Trainable params: 21,602,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Lip  Net Architecture\n",
    "model = Sequential()\n",
    "model.add(Conv3D(128, 3, input_shape=(timesteps,img_col,img_row,img_channel), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool3D((1,2,2)))\n",
    "\n",
    "model.add(Conv3D(256, 3, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool3D((1,2,2)))\n",
    "\n",
    "model.add(Conv3D(75, 3, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool3D((1,2,2)))\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model.add(Bidirectional(LSTM(128, kernel_initializer='Orthogonal', return_sequences=True)))\n",
    "model.add(Dropout(.5))\n",
    "\n",
    "model.add(Bidirectional(LSTM(128, kernel_initializer='Orthogonal', return_sequences=True)))\n",
    "model.add(Dropout(.5))\n",
    "\n",
    "model.add(Dense(n_labels+1, kernel_initializer='he_normal', activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture Visualization\n",
    "from IPython.display import SVG\n",
    "import pydot\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "%matplotlib inline\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))\n",
    "\n",
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Confirm the Learning Process\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "fig, loss_ax = plt.subplots()\n",
    "acc_ax = loss_ax.twinx()\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')  \n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "acc_ax.plot(hist.history['acc'], 'b', label='train acc')  \n",
    "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Using the Model\n",
    "model.save('Lib_Reading_10Frame_Model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lipreading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "13428ca5b44a4a9452cb06d4aa826f71385429b41d10a5b217fa3047f44ce11b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
